{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "007_model_with_text_preprocessing.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPaKefElDJjw",
        "colab_type": "text"
      },
      "source": [
        "# Text pre-processing  (part 2)\n",
        "\n",
        "Tensorflow is capable of building models for sentiment analysis, text summarization, translation etc.\n",
        "\n",
        "Today, we will complete this notebook, buiding the network  and training the model.\n",
        "\n",
        "On the previous notebook, we pre-processed text. Now, it's time to feed the network with it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEBGClIVSgtr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3a0eb59-dd51-4109-a5ee-0ea331d15d8b"
      },
      "source": [
        "# imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# get data\n",
        "!wget --no-check-certificate \\\n",
        "    -O /tmp/sentiment.csv https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P\n",
        "\n",
        "# define get_data function\n",
        "def get_data(path):\n",
        "  data= pd.read_csv(path, index_col=0)\n",
        "  return data\n",
        "\n",
        "#get the data\n",
        "data = get_data('/tmp/sentiment.csv')\n",
        "\n",
        "# clone package repository\n",
        "#!git clone https://github.com/vallantin/atalaia.git\n",
        "\n",
        "# navigate to atalaia directory\n",
        "#%cd atalaia\n",
        "\n",
        "# install packages requirements\n",
        "#!pip install -r requirements.txt\n",
        "\n",
        "# install package\n",
        "#!python setup.py install\n",
        "\n",
        "# import it\n",
        "from atalaia.atalaia import Atalaia\n",
        "\n",
        "#def pre-process function\n",
        "def preprocess(panda_series):\n",
        "  atalaia = Atalaia('en')\n",
        "\n",
        "  # lower case everyting and remove double spaces\n",
        "  panda_series = (atalaia.lower_remove_white(t) for t in panda_series)\n",
        "\n",
        "  # expand contractions\n",
        "  panda_series = (atalaia.expand_contractions(t) for t in panda_series)\n",
        "\n",
        "  # remove punctuation\n",
        "  panda_series = (atalaia.remove_punctuation(t) for t in panda_series)\n",
        "\n",
        "  # remove numbers\n",
        "  panda_series = (atalaia.remove_numbers(t) for t in panda_series)\n",
        "\n",
        "  # remove stopwords\n",
        "  panda_series = (atalaia.remove_stopwords(t) for t in panda_series)\n",
        "\n",
        "  # remove excessive spaces\n",
        "  panda_series = (atalaia.remove_excessive_spaces(t) for t in panda_series)\n",
        "\n",
        "  return panda_series\n",
        "\n",
        "# preprocess it\n",
        "preprocessed_text = preprocess(data.text)\n",
        "\n",
        "# assign preprocessed texts to dataset\n",
        "data['text']      = list(preprocessed_text)\n",
        "\n",
        "# split train/test\n",
        "# shuffle the dataset\n",
        "data = data.sample(frac=1)\n",
        "\n",
        "# separate all classes present on the dataset\n",
        "classes_dict = {}\n",
        "for label in [0,1]:\n",
        "  classes_dict[label] = data[data['sentiment'] == label]\n",
        "\n",
        "# get 80% of each label\n",
        "size = int(len(classes_dict[0].text) * 0.8)\n",
        "X_train = list(classes_dict[0].text[0:size])      + list(classes_dict[1].text[0:size])\n",
        "X_test  = list(classes_dict[0].text[size:])       + list(classes_dict[1].text[size:])\n",
        "y_train = list(classes_dict[0].sentiment[0:size]) + list(classes_dict[1].sentiment[0:size])\n",
        "y_test  = list(classes_dict[0].sentiment[size:])  + list(classes_dict[1].sentiment[size:])\n",
        "\n",
        "# Convert labels to Numpy arrays\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Let's consider the vocab size as the number of words\n",
        "# that compose 90% of the vocabulary\n",
        "atalaia    = Atalaia('en')\n",
        "vocab_size = len(atalaia.representative_tokens(0.9, \n",
        "                                               ' '.join(X_train),\n",
        "                                               reverse=False))\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "# start tokenize\n",
        "tokenizer = Tokenizer(num_words=vocab_size, \n",
        "                      oov_token=oov_tok)\n",
        "\n",
        "# fit on training\n",
        "# we don't fit on test because, in real life, our model will have to deal with\n",
        "# words ir never saw before. So, it makes sense fitting only on training.\n",
        "# when it finds a word it never saw before, it will assign the \n",
        "# <OOV> tag to it.\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# get the word index\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# transform into sequences\n",
        "# this will assign a index to the tokens present on the corpus\n",
        "sequences = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "# define max_length \n",
        "max_length = 100\n",
        "\n",
        "# post: pad or truncate after sentence.\n",
        "# pre: pad or truncate before sentence.\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "\n",
        "padded = pad_sequences(sequences,\n",
        "                       maxlen=max_length, \n",
        "                       padding=padding_type, \n",
        "                       truncating=trunc_type)\n",
        "\n",
        "# tokenize and pad test sentences\n",
        "# thse will be used later on the model for accuracy test\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_test_padded    = pad_sequences(X_test_sequences,\n",
        "                                 maxlen=max_length, \n",
        "                                 padding=padding_type, \n",
        "                                 truncating=trunc_type)\n",
        "\n",
        "# create the reverse word index\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "# create the decoder\n",
        "def text_decoder(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-03 09:08:50--  https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.24.138, 74.125.24.100, 74.125.24.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.24.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-08-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/uimvpv24vq1umlv1iubt44q28vep0rmb/1593767325000/11118900490791463723/*/13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-07-03 09:08:50--  https://doc-08-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/uimvpv24vq1umlv1iubt44q28vep0rmb/1593767325000/11118900490791463723/*/13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P\n",
            "Resolving doc-08-ak-docs.googleusercontent.com (doc-08-ak-docs.googleusercontent.com)... 74.125.130.132, 2404:6800:4003:c01::84\n",
            "Connecting to doc-08-ak-docs.googleusercontent.com (doc-08-ak-docs.googleusercontent.com)|74.125.130.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 127831 (125K) [text/csv]\n",
            "Saving to: ‘/tmp/sentiment.csv’\n",
            "\n",
            "/tmp/sentiment.csv  100%[===================>] 124.83K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2020-07-03 09:08:51 (118 MB/s) - ‘/tmp/sentiment.csv’ saved [127831/127831]\n",
            "\n",
            "Cloning into 'atalaia'...\n",
            "remote: Enumerating objects: 67, done.\u001b[K\n",
            "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 67 (delta 29), reused 59 (delta 21), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (67/67), done.\n",
            "/content/atalaia\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (3.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (4.41.1)\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (1.0.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (0.22.2.post1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 5)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 5)) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (0.15.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->-r requirements.txt (line 8)) (2.0.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (1.14.9)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (2.23.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.9 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (1.17.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (2020.6.20)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.9->boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (0.15.2)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=7b10d38b62f01f3c7284505b54253868bc2eb4bbeefeb8019f1da33b3060b9a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating Atalaia.egg-info\n",
            "writing Atalaia.egg-info/PKG-INFO\n",
            "writing dependency_links to Atalaia.egg-info/dependency_links.txt\n",
            "writing top-level names to Atalaia.egg-info/top_level.txt\n",
            "writing manifest file 'Atalaia.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "writing manifest file 'Atalaia.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/atalaia\n",
            "copying atalaia/vectors.py -> build/lib/atalaia\n",
            "copying atalaia/__init__.py -> build/lib/atalaia\n",
            "copying atalaia/strings.py -> build/lib/atalaia\n",
            "copying atalaia/files.py -> build/lib/atalaia\n",
            "copying atalaia/atalaia.py -> build/lib/atalaia\n",
            "creating build/lib/atalaia/assets\n",
            "copying atalaia/assets/stopwords.py -> build/lib/atalaia/assets\n",
            "copying atalaia/assets/__init__.py -> build/lib/atalaia/assets\n",
            "copying atalaia/assets/contractions.py -> build/lib/atalaia/assets\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/atalaia\n",
            "copying build/lib/atalaia/vectors.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "copying build/lib/atalaia/__init__.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "creating build/bdist.linux-x86_64/egg/atalaia/assets\n",
            "copying build/lib/atalaia/assets/stopwords.py -> build/bdist.linux-x86_64/egg/atalaia/assets\n",
            "copying build/lib/atalaia/assets/__init__.py -> build/bdist.linux-x86_64/egg/atalaia/assets\n",
            "copying build/lib/atalaia/assets/contractions.py -> build/bdist.linux-x86_64/egg/atalaia/assets\n",
            "copying build/lib/atalaia/strings.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "copying build/lib/atalaia/files.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "copying build/lib/atalaia/atalaia.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/vectors.py to vectors.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/assets/stopwords.py to stopwords.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/assets/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/assets/contractions.py to contractions.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/strings.py to strings.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/files.py to files.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/atalaia.py to atalaia.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Atalaia.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Atalaia.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Atalaia.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Atalaia.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/Atalaia-0.3.0-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing Atalaia-0.3.0-py3.6.egg\n",
            "Copying Atalaia-0.3.0-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding Atalaia 0.3.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/Atalaia-0.3.0-py3.6.egg\n",
            "Processing dependencies for Atalaia==0.3.0\n",
            "Finished processing dependencies for Atalaia==0.3.0\n",
            "X_train len is 1592\n",
            "y_train len is 1592\n",
            "X_test len is 400\n",
            "y_test len is 400\n",
            "furthermore you can not even find hours of operation on website\n",
            "0\n",
            "it should not take min for pancakes eggs\n",
            "0\n",
            "Decoded sentence:\n",
            "very slow at seating even reservation ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
            "\n",
            "Original sentence\n",
            "very slow at seating even reservation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQVkjY8v8N8X",
        "colab_type": "text"
      },
      "source": [
        "We will build a simple neural network with:\n",
        "- A Embedding layer\n",
        "- A Flatten layer\n",
        "- A first Dense layer\n",
        "- An output dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnBCIXmC8Kqn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "01a21811-33f2-43f1-ecc6-671f45a9bacf"
      },
      "source": [
        "# Build network\n",
        "embedding_dim = 16\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 16)           20448     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6)                 9606      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 7         \n",
            "=================================================================\n",
            "Total params: 30,061\n",
            "Trainable params: 30,061\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoMbZ5BB9zqv",
        "colab_type": "text"
      },
      "source": [
        "Now, it's time to fit and train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_5e90nT94xm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "5f090fca-a1ab-43db-920f-29f49398b37d"
      },
      "source": [
        "num_epochs = 10\n",
        "model.fit(padded, \n",
        "          y_train, \n",
        "          epochs=num_epochs, \n",
        "          validation_data=(X_test_padded, \n",
        "                           y_test))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.6940 - accuracy: 0.5025 - val_loss: 0.6926 - val_accuracy: 0.5325\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.6888 - accuracy: 0.5484 - val_loss: 0.6894 - val_accuracy: 0.5800\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.6736 - accuracy: 0.6106 - val_loss: 0.6732 - val_accuracy: 0.5900\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.6151 - accuracy: 0.7695 - val_loss: 0.6211 - val_accuracy: 0.6950\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.4870 - accuracy: 0.8555 - val_loss: 0.5506 - val_accuracy: 0.7425\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.3544 - accuracy: 0.9165 - val_loss: 0.4985 - val_accuracy: 0.7575\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2612 - accuracy: 0.9372 - val_loss: 0.4719 - val_accuracy: 0.7775\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.1945 - accuracy: 0.9567 - val_loss: 0.4597 - val_accuracy: 0.7900\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.1515 - accuracy: 0.9680 - val_loss: 0.4643 - val_accuracy: 0.7825\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.1205 - accuracy: 0.9793 - val_loss: 0.4476 - val_accuracy: 0.8025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f99a02a6208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWuoj-nP-VNA",
        "colab_type": "text"
      },
      "source": [
        "Now, it's time to check the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPYrzkl6-YXv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fb44d162-1dda-410c-ff4f-4398ea67592d"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(X_test_padded, y_test, verbose=2)\n",
        "print('\\nModel accuracy: {:.0f}%'.format(test_acc*100))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13/13 - 0s - loss: 0.4476 - accuracy: 0.8025\n",
            "\n",
            "Model accuracy: 80%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JyHebbd-YxP",
        "colab_type": "text"
      },
      "source": [
        "And do some predictions.\n",
        "\n",
        "Don't forget to pre-process the sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7J9ElFS-agc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a5ba2846-20b9-4520-e2d7-0f8ada276985"
      },
      "source": [
        "# Use the model to predict new reviews   \n",
        "new_reviews = ['Nothing could smell better than this fragrance.', \n",
        "               'Everything was perfect',\n",
        "               'They respect the environment.', \n",
        "               'The cake was a little dry',\n",
        "               'Everything was terrible.'\n",
        "               'it didn\\'t work as expected']\n",
        "\n",
        "# preprocess the texts\n",
        "new_reviews = list(preprocess(new_reviews))\n",
        "print(new_reviews)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['nothing could smell better than this fragrance', 'everything was perfect', 'they respect environment', 'cake was little dry', 'everything was terrible it did not work as expected']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlKNyj60Bstr",
        "colab_type": "text"
      },
      "source": [
        "Also, create the padded sequences for these new predictions.\n",
        "\n",
        "⚠️ Use the same configuration you used before. You also have to use the same tokenizer..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGNjQpQWBHn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the sequences\n",
        "padding_type     = 'post'\n",
        "new_sequences    = tokenizer.texts_to_sequences(new_reviews)\n",
        "new_padded       = pad_sequences(new_sequences, \n",
        "                                 padding=padding_type, \n",
        "                                 maxlen=max_length)           \n",
        "\n",
        "# predict\n",
        "y_pred           = model.predict(new_padded)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpZ0D9I6CR_J",
        "colab_type": "text"
      },
      "source": [
        "The predictions are on an array. Each element of the array is a probability of the sentence being positive or negative. The lesser the probability, the most negative the model thinks it is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE1lRc56CPJe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "3e7d0fa6-f70a-4010-fbc4-5b7858f9a6da"
      },
      "source": [
        "# See the predictions\n",
        "for x in range(len(new_reviews)):\n",
        "  print(new_reviews[x])\n",
        "  print(y_pred[x])\n",
        "  print('\\n')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nothing could smell better than this fragrance\n",
            "[0.3008928]\n",
            "\n",
            "\n",
            "everything was perfect\n",
            "[0.9252088]\n",
            "\n",
            "\n",
            "they respect environment\n",
            "[0.6564084]\n",
            "\n",
            "\n",
            "cake was little dry\n",
            "[0.05756388]\n",
            "\n",
            "\n",
            "everything was terrible it did not work as expected\n",
            "[0.01458555]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}