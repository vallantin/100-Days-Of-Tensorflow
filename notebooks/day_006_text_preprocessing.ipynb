{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "006_text_preprocessing",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPaKefElDJjw",
        "colab_type": "text"
      },
      "source": [
        "# Text pre-processing\n",
        "\n",
        "Tensorflow is capable of building models for sentiment analysis, text summarization, translation etc.\n",
        "\n",
        "But such as other types of data, text has to be pre-processed. \n",
        "\n",
        "Pre-processing is the same as normalizing your data. \n",
        "\n",
        "One example of text normalization is setting everything to lowercase. Example:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3s38zlzC9P3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "73f77668-9956-449a-da2f-a8af3f45cbf7"
      },
      "source": [
        "# for Python, \"Mary\" and \"mary\" are two different entities\n",
        "print('Is \"Mary\" the same as \"mary\"?')\n",
        "print('Mary' == 'mary')\n",
        "\n",
        "# now, let's remove the case of Mary\n",
        "print('Is \"Mary.lower()\" the same as \"mary\"?')\n",
        "print('Mary'.lower() == 'mary')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is \"Mary\" the same as \"mary\"?\n",
            "False\n",
            "Is \"Mary.lower()\" the same as \"mary\"?\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8PHUuhvSihv",
        "colab_type": "text"
      },
      "source": [
        "This is only one example of pre-processing. \n",
        "\n",
        "If you are  dealing with other language than English, you could remove accents, for example.\n",
        "\n",
        "## Do the imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEBGClIVSgtr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMxdkM0VUjxp",
        "colab_type": "text"
      },
      "source": [
        "## Get the data\n",
        "\n",
        "Let's use the Amazon and Yelp reviews dataset. You can see it [here](https://www.kaggle.com/marklvl/sentiment-labelled-sentences-data-set)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6SC6XG8SSIq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "c5864104-4a44-4041-a18d-1f59da062b19"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    -O /tmp/sentiment.csv https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-02 08:52:45--  https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.15.78, 2607:f8b0:4004:808::200e\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.15.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-08-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/p2f3793c848isrdbcpol9d245o7n5vvr/1593679950000/11118900490791463723/*/13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-07-02 08:52:46--  https://doc-08-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/p2f3793c848isrdbcpol9d245o7n5vvr/1593679950000/11118900490791463723/*/13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P\n",
            "Resolving doc-08-ak-docs.googleusercontent.com (doc-08-ak-docs.googleusercontent.com)... 142.250.31.132, 2607:f8b0:4004:c0b::84\n",
            "Connecting to doc-08-ak-docs.googleusercontent.com (doc-08-ak-docs.googleusercontent.com)|142.250.31.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 127831 (125K) [text/csv]\n",
            "Saving to: ‘/tmp/sentiment.csv’\n",
            "\n",
            "\r/tmp/sentiment.csv    0%[                    ]       0  --.-KB/s               \r/tmp/sentiment.csv  100%[===================>] 124.83K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-07-02 08:52:46 (5.36 MB/s) - ‘/tmp/sentiment.csv’ saved [127831/127831]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEhQH_FNVkMt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "99e1ffb5-46ee-41e9-e520-9dde8b238a57"
      },
      "source": [
        "# define get_data function\n",
        "def get_data(path):\n",
        "  data= pd.read_csv(path, index_col=0)\n",
        "  return data\n",
        "\n",
        "#get the data\n",
        "data = get_data('/tmp/sentiment.csv')\n",
        "\n",
        "#check the data\n",
        "data.head(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So there is no way for me to plug it in here i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Good case Excellent value.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Great for the jawbone.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tied to charger for conversations lasting more...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The mic is great.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  sentiment\n",
              "0  So there is no way for me to plug it in here i...          0\n",
              "1                         Good case Excellent value.          1\n",
              "2                             Great for the jawbone.          1\n",
              "3  Tied to charger for conversations lasting more...          0\n",
              "4                                  The mic is great.          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvWNYSF6Vl_D",
        "colab_type": "text"
      },
      "source": [
        "This dataset has 1992 reviews. It has 996 negative and positive reviews. \n",
        "\n",
        "Now, we can preprocess it using atalaia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8sjoOw1V1-i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "31c389f0-35b1-4a7a-9fce-20d67769362c"
      },
      "source": [
        "# clone package repository\n",
        "!git clone https://github.com/vallantin/atalaia.git\n",
        "\n",
        "# navigate to atalaia directory\n",
        "%cd atalaia\n",
        "\n",
        "# install packages requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# install package\n",
        "!python setup.py install\n",
        "\n",
        "# import it\n",
        "from atalaia.atalaia import Atalaia"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'atalaia' already exists and is not an empty directory.\n",
            "/content/atalaia\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (3.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (4.41.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.5.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (1.0.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (0.22.2.post1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 5)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 5)) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (0.15.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->-r requirements.txt (line 8)) (2.0.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (1.14.9)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (2.23.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.9 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (1.17.9)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (0.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.9->boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (0.15.2)\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing Atalaia.egg-info/PKG-INFO\n",
            "writing dependency_links to Atalaia.egg-info/dependency_links.txt\n",
            "writing top-level names to Atalaia.egg-info/top_level.txt\n",
            "reading manifest template 'MANIFEST.in'\n",
            "writing manifest file 'Atalaia.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/atalaia\n",
            "copying build/lib/atalaia/vectors.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "copying build/lib/atalaia/__init__.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "creating build/bdist.linux-x86_64/egg/atalaia/assets\n",
            "copying build/lib/atalaia/assets/stopwords.py -> build/bdist.linux-x86_64/egg/atalaia/assets\n",
            "copying build/lib/atalaia/assets/__init__.py -> build/bdist.linux-x86_64/egg/atalaia/assets\n",
            "copying build/lib/atalaia/assets/contractions.py -> build/bdist.linux-x86_64/egg/atalaia/assets\n",
            "copying build/lib/atalaia/strings.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "copying build/lib/atalaia/files.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "copying build/lib/atalaia/atalaia.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/vectors.py to vectors.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/assets/stopwords.py to stopwords.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/assets/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/assets/contractions.py to contractions.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/strings.py to strings.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/files.py to files.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/atalaia.py to atalaia.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Atalaia.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Atalaia.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Atalaia.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Atalaia.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating 'dist/Atalaia-0.3.0-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing Atalaia-0.3.0-py3.6.egg\n",
            "Removing /usr/local/lib/python3.6/dist-packages/Atalaia-0.3.0-py3.6.egg\n",
            "Copying Atalaia-0.3.0-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Atalaia 0.3.0 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/Atalaia-0.3.0-py3.6.egg\n",
            "Processing dependencies for Atalaia==0.3.0\n",
            "Finished processing dependencies for Atalaia==0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMfQ8KxMaM0R",
        "colab_type": "text"
      },
      "source": [
        "Our preprocess function will:\n",
        "\n",
        "- Lowercase everything\n",
        "- Expand contractions such as i'll -> i will\n",
        "- Remove punctuation from sentences. \n",
        "⚠️ Punctuation is sometimes important for sentiment analysis problems.\n",
        "- Remove numbers\n",
        "- Remove common stopwords such as 'the' from corpus\n",
        "- After removals, we can find blank spots on sentences. Let's remove these.\n",
        "\n",
        "Last step is assigning the pre-processed text to the corresponding dataset column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8B6k3vUW3h_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "87192bc0-9d15-440a-b1a2-d1855166f428"
      },
      "source": [
        "def preprocess(panda_series):\n",
        "  atalaia = Atalaia('en')\n",
        "\n",
        "  # lower case everyting and remove double spaces\n",
        "  panda_series = (atalaia.lower_remove_white(t) for t in panda_series)\n",
        "\n",
        "  # expand contractions\n",
        "  panda_series = (atalaia.expand_contractions(t) for t in panda_series)\n",
        "\n",
        "  # remove punctuation\n",
        "  panda_series = (atalaia.remove_punctuation(t) for t in panda_series)\n",
        "\n",
        "  # remove numbers\n",
        "  panda_series = (atalaia.remove_numbers(t) for t in panda_series)\n",
        "\n",
        "  # remove stopwords\n",
        "  panda_series = (atalaia.remove_stopwords(t) for t in panda_series)\n",
        "\n",
        "  # remove excessive spaces\n",
        "  panda_series = (atalaia.remove_excessive_spaces(t) for t in panda_series)\n",
        "\n",
        "  return panda_series\n",
        "\n",
        "# preprocess it\n",
        "preprocessed_text = preprocess(data.text)\n",
        "\n",
        "# assign preprocessed texts to dataset\n",
        "data['text']      = list(preprocessed_text)\n",
        "\n",
        "# see data\n",
        "data.head(5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>so there is no way for me plug it in here in u...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>good case excellent value</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>great for jawbone</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tied charger for conversations lasting more th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mic is great</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  sentiment\n",
              "0  so there is no way for me plug it in here in u...          0\n",
              "1                          good case excellent value          1\n",
              "2                                  great for jawbone          1\n",
              "3  tied charger for conversations lasting more th...          0\n",
              "4                                       mic is great          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3d1NgJmZYZf",
        "colab_type": "text"
      },
      "source": [
        "⚠️ Atalaia is a personal package I have created to studying purposes and is NOT READY FOR PRODUCTION. \n",
        "\n",
        "## Separate train and test\n",
        "\n",
        "Let's get 80% of the sentences for training and 20% for testing.\n",
        "\n",
        "Let's also try to keep the same amount of labels for each one of them:\n",
        "\n",
        " - 50% of negatives \n",
        " - 50% of positives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlZ2y94xYTVt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "2fc8f6eb-c08b-47f8-a5a7-a55954d133c6"
      },
      "source": [
        "# shuffle the dataset\n",
        "data = data.sample(frac=1)\n",
        "\n",
        "# separate all classes present on the dataset\n",
        "classes_dict = {}\n",
        "for label in [0,1]:\n",
        "  classes_dict[label] = data[data['sentiment'] == label]\n",
        "\n",
        "# get 80% of each label\n",
        "size = int(len(classes_dict[0].text) * 0.8)\n",
        "X_train = list(classes_dict[0].text[0:size])      + list(classes_dict[1].text[0:size])\n",
        "X_test  = list(classes_dict[0].text[size:])       + list(classes_dict[1].text[size:])\n",
        "y_train = list(classes_dict[0].sentiment[0:size]) + list(classes_dict[1].sentiment[0:size])\n",
        "y_test  = list(classes_dict[0].sentiment[size:])  + list(classes_dict[1].sentiment[size:])\n",
        "\n",
        "# print the lengths\n",
        "print('X_train len is {}'.format(len(X_train)))\n",
        "print('y_train len is {}'.format(len(y_train)))\n",
        "print('X_test len is {}'.format(len(X_test)))\n",
        "print('y_test len is {}'.format(len(y_test)))\n",
        "\n",
        "# print X_train first sentence and its label\n",
        "print(X_train[0])\n",
        "print(y_train[0])\n",
        "\n",
        "# print X_test first sentence and its label\n",
        "print(X_test[0])\n",
        "print(y_test[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train len is 1592\n",
            "y_train len is 1592\n",
            "X_test len is 400\n",
            "y_test len is 400\n",
            "i will be drivng along my headset starts ringing for no reason\n",
            "0\n",
            "chains which i am no fan of beat this place easily\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZahNbVGvkaiT",
        "colab_type": "text"
      },
      "source": [
        "In order to use this data for training, we need to convert the labels to Numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj5ATkiebnnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert labels to Numpy arrays\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfO7JW7bk-dq",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing\n",
        "\n",
        "The next step of the preprocessing phase is tokenizing. Tokenization is dividing a sentence into smaller pieces of information. Generally, we consider a word as being one token.\n",
        "\n",
        "Tensorflow provides a Tokenizer you can use right away. \n",
        "\n",
        "You must provide:\n",
        "\n",
        "1.   The size of the vocabulary to keep\n",
        "2.   A name that will be used to assign every token that is outside boundaries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEcivxBMlO5L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "239f1d08-597e-44b7-9ad8-05625882a6bb"
      },
      "source": [
        "# Let's consider the vocab size as the number of words\n",
        "# that compose 90% of the vocabulary\n",
        "atalaia    = Atalaia('en')\n",
        "vocab_size = len(atalaia.representative_tokens(0.9, \n",
        "                                               ' '.join(X_train),\n",
        "                                               reverse=False))\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "# start tokenize\n",
        "tokenizer = Tokenizer(num_words=vocab_size, \n",
        "                      oov_token=oov_tok)\n",
        "\n",
        "# fit on training\n",
        "# we don't fit on test because, in real life, our model will have to deal with\n",
        "# words ir never saw before. So, it makes sense fitting only on training.\n",
        "# when it finds a word it never saw before, it will assign the \n",
        "# <OOV> tag to it.\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# get the word index\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# transform into sequences\n",
        "# this will assign a index to the tokens present on the corpus\n",
        "sequences = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "# see the first sequence\n",
        "sequences[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 28, 27, 1215, 805, 9, 56, 806, 807, 10, 65, 808]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am-ZQhUBoJy8",
        "colab_type": "text"
      },
      "source": [
        "## Pad sentences\n",
        "\n",
        "Every sentence you use to feed your network has to have the same length. One sentence cannot be shorter (or longer) than another one.\n",
        "\n",
        "To make all sentences the same length, we pad them. \n",
        "\n",
        "We define a max length number. Every sentence longer than this number will be truncated. And every sentence shorter than this number will be padded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUGc8UUfoL7c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "93d7ae81-a915-4698-e8c3-29f5d8061491"
      },
      "source": [
        "# define max_length \n",
        "max_length = 100\n",
        "\n",
        "# post: pad or truncate after sentence.\n",
        "# pre: pad or truncate before sentence.\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "\n",
        "padded = pad_sequences(sequences,\n",
        "                       maxlen=max_length, \n",
        "                       padding=padding_type, \n",
        "                       truncating=trunc_type)\n",
        "\n",
        "# tokenize and pad test sentences\n",
        "# thse will be used later on the model for accuracy test\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_test_padded    = pad_sequences(X_test_sequences,\n",
        "                                 maxlen=max_length, \n",
        "                                 padding=padding_type, \n",
        "                                 truncating=trunc_type)\n",
        "\n",
        "# check the first padded sentence. Notice that 0s were added to it\n",
        "# because it was shorter than 100\n",
        "padded[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   2,   28,   27, 1215,  805,    9,   56,  806,  807,   10,   65,\n",
              "        808,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDcUK_EBsewL",
        "colab_type": "text"
      },
      "source": [
        "Today's last step is the creation of a \"decoder\".The decoder will read the padded sequence and will \"transform\" it to a real textual sentence. \n",
        "\n",
        "Our model will output sequences of numbers. We will need to translate them later to real sentences.\n",
        "\n",
        "We do it by creating a reverse word dict based on the word_index dictionary we just got after fitting the tokenizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2VtiouqsvP2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "d56692fa-60d6-4008-a58f-d73dee0f689c"
      },
      "source": [
        "# create the reverse word index\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "# create the decoder\n",
        "def text_decoder(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "# print the decoder output for one sentence and compare it to original\n",
        "print('Decoded sentence:')\n",
        "print(text_decoder(padded[1]))\n",
        "print('\\nOriginal sentence')\n",
        "print(X_train[1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoded sentence:\n",
            "i live in neighborhood so i am disappointed i will not be back here because it is convenient location ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
            "\n",
            "Original sentence\n",
            "i live in neighborhood so i am disappointed i will not be back here because it is convenient location\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}