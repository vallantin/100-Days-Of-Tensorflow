{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day_016_model_after_exploration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPaKefElDJjw",
        "colab_type": "text"
      },
      "source": [
        "# Retrain model\n",
        "\n",
        "Now that we have explored our dataset, let's do a few changes to it and retrain the model.\n",
        "\n",
        "We will:\n",
        "\n",
        "- Exclude the outlier sentence\n",
        "- Exclude stopwords\n",
        "\n",
        "By the way: I forgot to mention that I did a few changes on the atalaia package to match more stop words.\n",
        "\n",
        "In last exercise, we got the tokens that were part, at the same time, of the positive and negative sentences. There were 349 tokens in this situation (excluding stop words). The whole corpus (also excluding stop words) is composed by 3109 tokens.\n",
        "\n",
        "So, 11% of the tokens are present in both sets... What would happen if we exclude these ambiguous tokens? Could this help our model to better generalize?\n",
        "\n",
        "Remember: the goal of this challenge is testing and exploring possibilities. Let's see where this leads us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEBGClIVSgtr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "220a79c2-014c-469e-e983-a0742b676400"
      },
      "source": [
        "# imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import numpy as np\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import percentile\n",
        "import pandas as pd\n",
        "import scipy\n",
        "\n",
        "# get data\n",
        "!wget --no-check-certificate \\\n",
        "    -O /tmp/sentiment.csv https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P\n",
        "\n",
        "# define get_data function\n",
        "def get_data(path):\n",
        "  data = pd.read_csv(path, index_col=0)\n",
        "  return data\n",
        "\n",
        "#get the data\n",
        "data = get_data('/tmp/sentiment.csv')\n",
        "\n",
        "# clone package repository\n",
        "!git clone https://github.com/vallantin/atalaia.git\n",
        "\n",
        "# navigate to atalaia directory\n",
        "%cd atalaia\n",
        "\n",
        "# install packages requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# install package\n",
        "!python setup.py install\n",
        "\n",
        "# import it\n",
        "from atalaia.atalaia import Atalaia\n",
        "\n",
        "# get a list with all the texts\n",
        "texts = data.text\n",
        "\n",
        "#start atalaia\n",
        "atalaia = Atalaia('en')\n",
        "\n",
        "# get the number of tokens in each sentence\n",
        "# get the lengths\n",
        "lens = [len(atalaia.tokenize(t)) for t in texts]\n",
        "data['lengths'] = lens\n",
        "\n",
        "#delete outliers\n",
        "data = data.drop(index = [1228])\n",
        "\n",
        "# lower everything\n",
        "data['text'] = [atalaia.lower_remove_white(t) for t in data['text']]\n",
        "\n",
        "# exclude expand contractions\n",
        "data['text'] = [atalaia.expand_contractions(t) for t in data['text']]\n",
        "\n",
        "# exclude punctuation\n",
        "data['text'] = [atalaia.remove_punctuation(t) for t in data['text']]\n",
        "\n",
        "# exclude numbers\n",
        "data['text'] = [atalaia.remove_numbers(t) for t in data['text']]\n",
        "\n",
        "# exclude stopwords\n",
        "data['text'] = [atalaia.remove_stopwords(t) for t in data['text']]\n",
        "\n",
        "# exclude excessive spaces\n",
        "data['text'] = [atalaia.remove_excessive_spaces(t) for t in data['text']]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-14 14:31:43--  https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.142.113, 74.125.142.138, 74.125.142.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.142.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-08-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/48v2ml670t8h85e8padoec1hhn45o104/1594737075000/11118900490791463723/*/13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-07-14 14:31:43--  https://doc-08-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/48v2ml670t8h85e8padoec1hhn45o104/1594737075000/11118900490791463723/*/13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P\n",
            "Resolving doc-08-ak-docs.googleusercontent.com (doc-08-ak-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-08-ak-docs.googleusercontent.com (doc-08-ak-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 127831 (125K) [text/csv]\n",
            "Saving to: ‘/tmp/sentiment.csv’\n",
            "\n",
            "/tmp/sentiment.csv  100%[===================>] 124.83K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2020-07-14 14:31:43 (64.4 MB/s) - ‘/tmp/sentiment.csv’ saved [127831/127831]\n",
            "\n",
            "Cloning into 'atalaia'...\n",
            "remote: Enumerating objects: 104, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 104 (delta 62), reused 81 (delta 39), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (104/104), 40.65 KiB | 8.13 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n",
            "/content/atalaia\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (3.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (4.41.1)\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (1.0.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (0.22.2.post1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 5)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 5)) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (0.16.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->-r requirements.txt (line 8)) (2.1.0)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (1.14.18)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (2.23.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.18 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (1.17.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (2.10)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.18->boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (0.15.2)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=b5a7b4205e9dc0bc9596391508f1ebea7a62f0fcff5755bc8224d1fa5f5106b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating Atalaia.egg-info\n",
            "writing Atalaia.egg-info/PKG-INFO\n",
            "writing dependency_links to Atalaia.egg-info/dependency_links.txt\n",
            "writing top-level names to Atalaia.egg-info/top_level.txt\n",
            "writing manifest file 'Atalaia.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "writing manifest file 'Atalaia.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/atalaia\n",
            "copying atalaia/vectors.py -> build/lib/atalaia\n",
            "copying atalaia/atalaia.py -> build/lib/atalaia\n",
            "copying atalaia/strings.py -> build/lib/atalaia\n",
            "copying atalaia/files.py -> build/lib/atalaia\n",
            "copying atalaia/__init__.py -> build/lib/atalaia\n",
            "creating build/lib/atalaia/assets\n",
            "copying atalaia/assets/contractions.py -> build/lib/atalaia/assets\n",
            "copying atalaia/assets/stopwords.py -> build/lib/atalaia/assets\n",
            "copying atalaia/assets/__init__.py -> build/lib/atalaia/assets\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/atalaia\n",
            "copying build/lib/atalaia/vectors.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "copying build/lib/atalaia/atalaia.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "copying build/lib/atalaia/strings.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "copying build/lib/atalaia/files.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "creating build/bdist.linux-x86_64/egg/atalaia/assets\n",
            "copying build/lib/atalaia/assets/contractions.py -> build/bdist.linux-x86_64/egg/atalaia/assets\n",
            "copying build/lib/atalaia/assets/stopwords.py -> build/bdist.linux-x86_64/egg/atalaia/assets\n",
            "copying build/lib/atalaia/assets/__init__.py -> build/bdist.linux-x86_64/egg/atalaia/assets\n",
            "copying build/lib/atalaia/__init__.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/vectors.py to vectors.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/atalaia.py to atalaia.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/strings.py to strings.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/files.py to files.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/assets/contractions.py to contractions.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/assets/stopwords.py to stopwords.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/assets/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/__init__.py to __init__.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Atalaia.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Atalaia.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Atalaia.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Atalaia.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/Atalaia-0.3.4-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing Atalaia-0.3.4-py3.6.egg\n",
            "Copying Atalaia-0.3.4-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding Atalaia 0.3.4 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/Atalaia-0.3.4-py3.6.egg\n",
            "Processing dependencies for Atalaia==0.3.4\n",
            "Finished processing dependencies for Atalaia==0.3.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqLSzGKtXXvA",
        "colab_type": "text"
      },
      "source": [
        "Get the intersection tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nlJ8Kp8XWJ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "eae88265-f5b6-417c-8aa4-be787bd6abda"
      },
      "source": [
        "# create function to get positive and negative tokens\n",
        "def representative_words_for_sentiment(sentences, data):\n",
        "  # start atalaia\n",
        "  atalaia = Atalaia('en')\n",
        "\n",
        "  # transform into corpus\n",
        "  sentences = atalaia.create_corpus(sentences)\n",
        "\n",
        "  # get the representative words for 80% of the corpus\n",
        "  token_data = atalaia.representative_tokens(0.8, \n",
        "                                            sentences,\n",
        "                                            reverse=False)\n",
        "\n",
        "  full_token_data                     = token_data.items()\n",
        "  full_token_data_tokens, full_counts = zip(*full_token_data)\n",
        "\n",
        "  token_data                          = list(full_token_data)[:10]\n",
        "  tokens, counts                      = zip(*token_data)\n",
        "\n",
        "  # return tokens list\n",
        "  return full_token_data_tokens\n",
        "\n",
        "# get positive sentences\n",
        "positive        = list(data[data.sentiment  == 1]['text'])\n",
        "positive_tokens = representative_words_for_sentiment(positive,data)\n",
        "\n",
        "# get negative sentences\n",
        "negative        = list(data[data.sentiment  == 0]['text'])\n",
        "negative_tokens = representative_words_for_sentiment(negative,data)\n",
        "\n",
        "#get the intersection of negative and positive tokens\n",
        "intersection = list(set(positive_tokens) & set(negative_tokens))\n",
        "intersection[:10]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['seated',\n",
              " 'customer',\n",
              " 'lunch',\n",
              " 'pho',\n",
              " 'dishes',\n",
              " 'bluetooth',\n",
              " 'piece',\n",
              " 'ears',\n",
              " 'friend',\n",
              " 'minutes']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn9C2yZU50m7",
        "colab_type": "text"
      },
      "source": [
        "Now, let's remove these tokens from the sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JUAgdPV8KbW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b2ee9ccc-05ff-4791-c799-ca2dca41eef3"
      },
      "source": [
        "def exclude_intersection_tokens(sentence, intersection):\n",
        "  atalaia = Atalaia('en')\n",
        "  sentence = [token for token in atalaia.tokenize(sentence) if token not in intersection]\n",
        "  return ' '.join(sentence)\n",
        "\n",
        "# get the sentences without these insersection tokens\n",
        "preprocessed = [exclude_intersection_tokens(sentence, intersection) for sentence in data.text]\n",
        "preprocessed[:10]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['unless converter',\n",
              " 'excellent value',\n",
              " 'jawbone',\n",
              " 'tied conversations lasting major',\n",
              " '',\n",
              " 'jiggle',\n",
              " 'dozen hundred contacts imagine',\n",
              " 'owner',\n",
              " 'needless wasted money',\n",
              " 'waste money']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkBqAWuxTK6B",
        "colab_type": "text"
      },
      "source": [
        "Now, we have a problem: some of the sentences are empty... It seems that we will have to work with these tokens anyway.\n",
        "\n",
        "Finally, we got back to the starting point... The only real change we did was augmenting the number of stop words matched and excluding one outlier. \n",
        "\n",
        "Let's split the data into train and test again and retrain the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEnmFVfVSeXp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "49b794a8-a960-4130-8f47-f2f215a24198"
      },
      "source": [
        "# split train/test\n",
        "# shuffle the dataset\n",
        "data = data.sample(frac=1)\n",
        "\n",
        "# separate all classes present on the dataset\n",
        "classes_dict = {}\n",
        "for label in [0,1]:\n",
        "  classes_dict[label] = data[data['sentiment'] == label]\n",
        "\n",
        "# get 80% of each label\n",
        "size = int(len(classes_dict[0].text) * 0.8)\n",
        "X_train = list(classes_dict[0].text[0:size])      + list(classes_dict[1].text[0:size])\n",
        "X_test  = list(classes_dict[0].text[size:])       + list(classes_dict[1].text[size:])\n",
        "y_train = list(classes_dict[0].sentiment[0:size]) + list(classes_dict[1].sentiment[0:size])\n",
        "y_test  = list(classes_dict[0].sentiment[size:])  + list(classes_dict[1].sentiment[size:])\n",
        "\n",
        "# Convert labels to Numpy arrays\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Let's consider the vocab size as the number of words\n",
        "# that compose 90% of the vocabulary\n",
        "atalaia    = Atalaia('en')\n",
        "vocab_size = len(atalaia.representative_tokens(0.9, \n",
        "                                               ' '.join(X_train),\n",
        "                                               reverse=False))\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "# start tokenize\n",
        "tokenizer = Tokenizer(num_words=vocab_size, \n",
        "                      oov_token=oov_tok)\n",
        "\n",
        "# fit on training\n",
        "# we don't fit on test because, in real life, our model will have to deal with\n",
        "# words ir never saw before. So, it makes sense fitting only on training.\n",
        "# when it finds a word it never saw before, it will assign the \n",
        "# <OOV> tag to it.\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# get the word index\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# transform into sequences\n",
        "# this will assign a index to the tokens present on the corpus\n",
        "sequences = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "# define max_length \n",
        "max_length = 100\n",
        "\n",
        "# post: pad or truncate after sentence.\n",
        "# pre: pad or truncate before sentence.\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "\n",
        "padded = pad_sequences(sequences,\n",
        "                       maxlen=max_length, \n",
        "                       padding=padding_type, \n",
        "                       truncating=trunc_type)\n",
        "\n",
        "# tokenize and pad test sentences\n",
        "# thse will be used later on the model for accuracy test\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_test_padded    = pad_sequences(X_test_sequences,\n",
        "                                 maxlen=max_length, \n",
        "                                 padding=padding_type, \n",
        "                                 truncating=trunc_type)\n",
        "\n",
        "# create the reverse word index\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "# create the decoder\n",
        "def text_decoder(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "# Build network\n",
        "embedding_dim = 16\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# train the model\n",
        "num_epochs = 10\n",
        "model.fit(padded, \n",
        "          y_train, \n",
        "          epochs=num_epochs, \n",
        "          validation_data=(X_test_padded, \n",
        "                           y_test))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 16)           27984     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6)                 9606      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 7         \n",
            "=================================================================\n",
            "Total params: 37,597\n",
            "Trainable params: 37,597\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.6940 - accuracy: 0.4837 - val_loss: 0.6932 - val_accuracy: 0.4987\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 0s 7ms/step - loss: 0.6929 - accuracy: 0.5000 - val_loss: 0.6930 - val_accuracy: 0.5013\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.6912 - accuracy: 0.5251 - val_loss: 0.6909 - val_accuracy: 0.6241\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.6547 - accuracy: 0.8090 - val_loss: 0.6461 - val_accuracy: 0.6942\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.4781 - accuracy: 0.8938 - val_loss: 0.6227 - val_accuracy: 0.6967\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.3217 - accuracy: 0.9102 - val_loss: 0.6119 - val_accuracy: 0.7243\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.2403 - accuracy: 0.9315 - val_loss: 0.5950 - val_accuracy: 0.7469\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1902 - accuracy: 0.9485 - val_loss: 0.6194 - val_accuracy: 0.7343\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1551 - accuracy: 0.9592 - val_loss: 0.5439 - val_accuracy: 0.7644\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1287 - accuracy: 0.9673 - val_loss: 0.5202 - val_accuracy: 0.7794\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbcb24b4320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WPMN2N6Db1w",
        "colab_type": "text"
      },
      "source": [
        "Get the predictions and analyse the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXak3DLHYAgl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "a060f580-c398-4900-b815-40fb86875446"
      },
      "source": [
        "# predict\n",
        "y_pred = model.predict(X_test_padded)\n",
        "\n",
        "# round\n",
        "y_pred =[1 if y > 0.5 else 0 for y in y_pred]\n",
        "\n",
        "# confusion matrix\n",
        "matrix = tf.math.confusion_matrix(y_test, \n",
        "                                  y_pred)\n",
        "\n",
        "matrix = np.array(matrix)\n",
        "\n",
        "matrix = pd.DataFrame(matrix, \n",
        "                      columns=['Positive (real)', 'Negative (real)'],\n",
        "                      index=['Positive (predicted)', 'Negative (predicted)'])\n",
        "\n",
        "matrix"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Positive (real)</th>\n",
              "      <th>Negative (real)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Positive (predicted)</th>\n",
              "      <td>132</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Negative (predicted)</th>\n",
              "      <td>21</td>\n",
              "      <td>179</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Positive (real)  Negative (real)\n",
              "Positive (predicted)              132               67\n",
              "Negative (predicted)               21              179"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2pQ0hXtuAW-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c7fc4f81-50aa-4c99-cd13-b5e480a942a3"
      },
      "source": [
        "# accuracy\n",
        "test_loss, test_acc = model.evaluate(X_test_padded, y_test, verbose=2)\n",
        "print('\\nModel accuracy: {:.0f}%'.format(test_acc*100))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13/13 - 0s - loss: 0.5202 - accuracy: 0.7794\n",
            "\n",
            "Model accuracy: 78%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}