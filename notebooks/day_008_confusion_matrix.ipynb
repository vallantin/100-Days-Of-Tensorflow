{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day_008_conffusion_matrix.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPaKefElDJjw",
        "colab_type": "text"
      },
      "source": [
        "# The Confusion Matrix\n",
        "\n",
        "Accuracy is a fine way to understand performance, but there's an even better way to do it: the confusion matrix.\n",
        "\n",
        "This matrix makes easier to compare real values with the predicted ones. So you can spot false negatives and false positives.\n",
        "\n",
        "To start, let's reload the packages and data, preprocess it and retrain the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEBGClIVSgtr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "37aed5aa-33d3-4143-de6f-d6b6c29e0282"
      },
      "source": [
        "# imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# get data\n",
        "!wget --no-check-certificate \\\n",
        "    -O /tmp/sentiment.csv https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P\n",
        "\n",
        "# define get_data function\n",
        "def get_data(path):\n",
        "  data= pd.read_csv(path, index_col=0)\n",
        "  return data\n",
        "\n",
        "#get the data\n",
        "data = get_data('/tmp/sentiment.csv')\n",
        "\n",
        "# clone package repository\n",
        "!git clone https://github.com/vallantin/atalaia.git\n",
        "\n",
        "# navigate to atalaia directory\n",
        "%cd atalaia\n",
        "\n",
        "# install packages requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# install package\n",
        "!python setup.py install\n",
        "\n",
        "# import it\n",
        "from atalaia.atalaia import Atalaia\n",
        "\n",
        "#def pre-process function\n",
        "def preprocess(panda_series):\n",
        "  atalaia = Atalaia('en')\n",
        "\n",
        "  # lower case everyting and remove double spaces\n",
        "  panda_series = (atalaia.lower_remove_white(t) for t in panda_series)\n",
        "\n",
        "  # expand contractions\n",
        "  panda_series = (atalaia.expand_contractions(t) for t in panda_series)\n",
        "\n",
        "  # remove punctuation\n",
        "  panda_series = (atalaia.remove_punctuation(t) for t in panda_series)\n",
        "\n",
        "  # remove numbers\n",
        "  panda_series = (atalaia.remove_numbers(t) for t in panda_series)\n",
        "\n",
        "  # remove stopwords\n",
        "  panda_series = (atalaia.remove_stopwords(t) for t in panda_series)\n",
        "\n",
        "  # remove excessive spaces\n",
        "  panda_series = (atalaia.remove_excessive_spaces(t) for t in panda_series)\n",
        "\n",
        "  return panda_series\n",
        "\n",
        "# preprocess it\n",
        "preprocessed_text = preprocess(data.text)\n",
        "\n",
        "# assign preprocessed texts to dataset\n",
        "data['text']      = list(preprocessed_text)\n",
        "\n",
        "# split train/test\n",
        "# shuffle the dataset\n",
        "data = data.sample(frac=1)\n",
        "\n",
        "# separate all classes present on the dataset\n",
        "classes_dict = {}\n",
        "for label in [0,1]:\n",
        "  classes_dict[label] = data[data['sentiment'] == label]\n",
        "\n",
        "# get 80% of each label\n",
        "size = int(len(classes_dict[0].text) * 0.8)\n",
        "X_train = list(classes_dict[0].text[0:size])      + list(classes_dict[1].text[0:size])\n",
        "X_test  = list(classes_dict[0].text[size:])       + list(classes_dict[1].text[size:])\n",
        "y_train = list(classes_dict[0].sentiment[0:size]) + list(classes_dict[1].sentiment[0:size])\n",
        "y_test  = list(classes_dict[0].sentiment[size:])  + list(classes_dict[1].sentiment[size:])\n",
        "\n",
        "# Convert labels to Numpy arrays\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Let's consider the vocab size as the number of words\n",
        "# that compose 90% of the vocabulary\n",
        "atalaia    = Atalaia('en')\n",
        "vocab_size = len(atalaia.representative_tokens(0.9, \n",
        "                                               ' '.join(X_train),\n",
        "                                               reverse=False))\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "# start tokenize\n",
        "tokenizer = Tokenizer(num_words=vocab_size, \n",
        "                      oov_token=oov_tok)\n",
        "\n",
        "# fit on training\n",
        "# we don't fit on test because, in real life, our model will have to deal with\n",
        "# words ir never saw before. So, it makes sense fitting only on training.\n",
        "# when it finds a word it never saw before, it will assign the \n",
        "# <OOV> tag to it.\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# get the word index\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# transform into sequences\n",
        "# this will assign a index to the tokens present on the corpus\n",
        "sequences = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "# define max_length \n",
        "max_length = 100\n",
        "\n",
        "# post: pad or truncate after sentence.\n",
        "# pre: pad or truncate before sentence.\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "\n",
        "padded = pad_sequences(sequences,\n",
        "                       maxlen=max_length, \n",
        "                       padding=padding_type, \n",
        "                       truncating=trunc_type)\n",
        "\n",
        "# tokenize and pad test sentences\n",
        "# thse will be used later on the model for accuracy test\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_test_padded    = pad_sequences(X_test_sequences,\n",
        "                                 maxlen=max_length, \n",
        "                                 padding=padding_type, \n",
        "                                 truncating=trunc_type)\n",
        "\n",
        "# create the reverse word index\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "# create the decoder\n",
        "def text_decoder(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "# Build network\n",
        "embedding_dim = 16\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# train the model\n",
        "num_epochs = 10\n",
        "model.fit(padded, \n",
        "          y_train, \n",
        "          epochs=num_epochs, \n",
        "          validation_data=(X_test_padded, \n",
        "                           y_test))\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-04 08:01:05--  https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P\n",
            "Resolving drive.google.com (drive.google.com)... 64.233.184.139, 64.233.184.138, 64.233.184.113, ...\n",
            "Connecting to drive.google.com (drive.google.com)|64.233.184.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-08-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4qqphkv2ncu3cef211ffsoc3vun3smvi/1593849600000/11118900490791463723/*/13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-07-04 08:01:05--  https://doc-08-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4qqphkv2ncu3cef211ffsoc3vun3smvi/1593849600000/11118900490791463723/*/13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P\n",
            "Resolving doc-08-ak-docs.googleusercontent.com (doc-08-ak-docs.googleusercontent.com)... 142.250.13.132, 2a00:1450:400c:c03::84\n",
            "Connecting to doc-08-ak-docs.googleusercontent.com (doc-08-ak-docs.googleusercontent.com)|142.250.13.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 127831 (125K) [text/csv]\n",
            "Saving to: ‘/tmp/sentiment.csv’\n",
            "\n",
            "\r/tmp/sentiment.csv    0%[                    ]       0  --.-KB/s               \r/tmp/sentiment.csv  100%[===================>] 124.83K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2020-07-04 08:01:05 (98.6 MB/s) - ‘/tmp/sentiment.csv’ saved [127831/127831]\n",
            "\n",
            "Cloning into 'atalaia'...\n",
            "remote: Enumerating objects: 67, done.\u001b[K\n",
            "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 67 (delta 29), reused 59 (delta 21), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (67/67), done.\n",
            "/content/atalaia\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (3.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (4.41.1)\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (1.0.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (0.22.2.post1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 5)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 5)) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (0.15.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->-r requirements.txt (line 8)) (2.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (2.23.0)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (1.14.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (2.9)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.9 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (1.17.9)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.9->boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (0.15.2)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=86af658b0070740dcfaf31f5a8a8a0dc6ba4a486147c25ca9e7bfff0b23a9f10\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating Atalaia.egg-info\n",
            "writing Atalaia.egg-info/PKG-INFO\n",
            "writing dependency_links to Atalaia.egg-info/dependency_links.txt\n",
            "writing top-level names to Atalaia.egg-info/top_level.txt\n",
            "writing manifest file 'Atalaia.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "writing manifest file 'Atalaia.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/atalaia\n",
            "copying atalaia/vectors.py -> build/lib/atalaia\n",
            "copying atalaia/__init__.py -> build/lib/atalaia\n",
            "copying atalaia/strings.py -> build/lib/atalaia\n",
            "copying atalaia/files.py -> build/lib/atalaia\n",
            "copying atalaia/atalaia.py -> build/lib/atalaia\n",
            "creating build/lib/atalaia/assets\n",
            "copying atalaia/assets/stopwords.py -> build/lib/atalaia/assets\n",
            "copying atalaia/assets/__init__.py -> build/lib/atalaia/assets\n",
            "copying atalaia/assets/contractions.py -> build/lib/atalaia/assets\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/atalaia\n",
            "copying build/lib/atalaia/vectors.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "copying build/lib/atalaia/__init__.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "creating build/bdist.linux-x86_64/egg/atalaia/assets\n",
            "copying build/lib/atalaia/assets/stopwords.py -> build/bdist.linux-x86_64/egg/atalaia/assets\n",
            "copying build/lib/atalaia/assets/__init__.py -> build/bdist.linux-x86_64/egg/atalaia/assets\n",
            "copying build/lib/atalaia/assets/contractions.py -> build/bdist.linux-x86_64/egg/atalaia/assets\n",
            "copying build/lib/atalaia/strings.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "copying build/lib/atalaia/files.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "copying build/lib/atalaia/atalaia.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/vectors.py to vectors.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/assets/stopwords.py to stopwords.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/assets/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/assets/contractions.py to contractions.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/strings.py to strings.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/files.py to files.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/atalaia.py to atalaia.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Atalaia.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Atalaia.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Atalaia.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Atalaia.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/Atalaia-0.3.0-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing Atalaia-0.3.0-py3.6.egg\n",
            "Copying Atalaia-0.3.0-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding Atalaia 0.3.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/Atalaia-0.3.0-py3.6.egg\n",
            "Processing dependencies for Atalaia==0.3.0\n",
            "Finished processing dependencies for Atalaia==0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoMbZ5BB9zqv",
        "colab_type": "text"
      },
      "source": [
        "Instead of checking accuracy, we will create our confusion matrix. First thing to do is to do some predictions. We will reuse the test set, but if you have enough data, you could use a validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnIJ6JAY2PP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict\n",
        "y_pred = model.predict(X_test_padded)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoMfd9RD5HoA",
        "colab_type": "text"
      },
      "source": [
        "Predictions are in a probabilistic format. But our labels are in a 0 or 1 format. Let's round our predictions and assume that if they are greater than 0.5, they are positive and if they are lesser than this number, they are negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgcEobPl5H9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# round\n",
        "y_pred =[1 if y > 0.5 else 0 for y in y_pred]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz2CA8pj2F_r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "b70565c6-c7d5-4e15-efa6-1e72f9e9abd8"
      },
      "source": [
        "# confusion matrix\n",
        "matrix = tf.math.confusion_matrix(y_test, \n",
        "                                  y_pred)\n",
        "\n",
        "matrix = np.array(matrix)\n",
        "\n",
        "matrix = pd.DataFrame(matrix, \n",
        "                      columns=['Positive (real)', 'Negative (real)'],\n",
        "                      index=['Positive (predicted)', 'Negative (predicted)'])\n",
        "\n",
        "matrix"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Positive (real)</th>\n",
              "      <th>Negative (real)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Positive (predicted)</th>\n",
              "      <td>165</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Negative (predicted)</th>\n",
              "      <td>51</td>\n",
              "      <td>149</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Positive (real)  Negative (real)\n",
              "Positive (predicted)              165               35\n",
              "Negative (predicted)               51              149"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb9UYeHi5eFw",
        "colab_type": "text"
      },
      "source": [
        "And voilà our matrix. The first \"cell\" shows the good Positive predictions, while the second shows false-positives (they were negative, but model predicted them as positive).\n",
        "\n",
        "On the second line, the first cell shows the false-negatives (they were positive, but model predicted them as negative), while the next cell shows the real negatives. \n",
        "\n",
        "False positives and false negatives are a big problem if you are on health industry (but not only...). Imagine you are training a model to detect cancer... a false positive is a headache, but a false negative could cause the death of someone.\n",
        "\n"
      ]
    }
  ]
}