{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day_009_model_improvement.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPaKefElDJjw",
        "colab_type": "text"
      },
      "source": [
        "# Visualization with Embedding Projector\n",
        "\n",
        "Last time, I have talked about the importance of knowing what your model is doing.\n",
        "\n",
        "Tensorflow has the Embedding Projector tool, which lets you see how the model groups information in a visual way.\n",
        "\n",
        "Today, we will load the weights and the words found on Amazon reviews and see how they are grouped by the model.\n",
        "\n",
        "## Redo the pre-processing and model training. \n",
        "\n",
        "Retrain everything, but use 100 dimensions for your vectors this time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEBGClIVSgtr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b8d7a0fb-d83f-49ca-d9c3-3b5f339fb726"
      },
      "source": [
        "# imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import os\n",
        "import io\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# get data\n",
        "!wget --no-check-certificate \\\n",
        "    -O /tmp/sentiment.csv https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P\n",
        "\n",
        "# define get_data function\n",
        "def get_data(path):\n",
        "  data= pd.read_csv(path, index_col=0)\n",
        "  return data\n",
        "\n",
        "#get the data\n",
        "data = get_data('/tmp/sentiment.csv')\n",
        "\n",
        "# clone package repository\n",
        "!git clone https://github.com/vallantin/atalaia.git\n",
        "\n",
        "# navigate to atalaia directory\n",
        "%cd atalaia\n",
        "\n",
        "# install packages requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# install package\n",
        "!python setup.py install\n",
        "\n",
        "# import it\n",
        "from atalaia.atalaia import Atalaia\n",
        "\n",
        "#def pre-process function\n",
        "def preprocess(panda_series):\n",
        "  atalaia = Atalaia('en')\n",
        "\n",
        "  # lower case everyting and remove double spaces\n",
        "  panda_series = (atalaia.lower_remove_white(t) for t in panda_series)\n",
        "\n",
        "  # expand contractions\n",
        "  panda_series = (atalaia.expand_contractions(t) for t in panda_series)\n",
        "\n",
        "  # remove punctuation\n",
        "  panda_series = (atalaia.remove_punctuation(t) for t in panda_series)\n",
        "\n",
        "  # remove numbers\n",
        "  panda_series = (atalaia.remove_numbers(t) for t in panda_series)\n",
        "\n",
        "  # remove stopwords\n",
        "  panda_series = (atalaia.remove_stopwords(t) for t in panda_series)\n",
        "\n",
        "  # remove excessive spaces\n",
        "  panda_series = (atalaia.remove_excessive_spaces(t) for t in panda_series)\n",
        "\n",
        "  return panda_series\n",
        "\n",
        "# preprocess it\n",
        "preprocessed_text = preprocess(data.text)\n",
        "\n",
        "# assign preprocessed texts to dataset\n",
        "data['text']      = list(preprocessed_text)\n",
        "\n",
        "# split train/test\n",
        "# shuffle the dataset\n",
        "data = data.sample(frac=1)\n",
        "\n",
        "# separate all classes present on the dataset\n",
        "classes_dict = {}\n",
        "for label in [0,1]:\n",
        "  classes_dict[label] = data[data['sentiment'] == label]\n",
        "\n",
        "# get 80% of each label\n",
        "size = int(len(classes_dict[0].text) * 0.8)\n",
        "X_train = list(classes_dict[0].text[0:size])      + list(classes_dict[1].text[0:size])\n",
        "X_test  = list(classes_dict[0].text[size:])       + list(classes_dict[1].text[size:])\n",
        "y_train = list(classes_dict[0].sentiment[0:size]) + list(classes_dict[1].sentiment[0:size])\n",
        "y_test  = list(classes_dict[0].sentiment[size:])  + list(classes_dict[1].sentiment[size:])\n",
        "\n",
        "# Convert labels to Numpy arrays\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Let's consider the vocab size as the number of words\n",
        "# that compose 90% of the vocabulary\n",
        "atalaia    = Atalaia('en')\n",
        "vocab_size = len(atalaia.representative_tokens(0.9, \n",
        "                                               ' '.join(X_train),\n",
        "                                               reverse=False))\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "# start tokenize\n",
        "tokenizer = Tokenizer(num_words=vocab_size, \n",
        "                      oov_token=oov_tok)\n",
        "\n",
        "# fit on training\n",
        "# we don't fit on test because, in real life, our model will have to deal with\n",
        "# words ir never saw before. So, it makes sense fitting only on training.\n",
        "# when it finds a word it never saw before, it will assign the \n",
        "# <OOV> tag to it.\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# get the word index\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# transform into sequences\n",
        "# this will assign a index to the tokens present on the corpus\n",
        "sequences = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "# define max_length \n",
        "max_length = 100\n",
        "\n",
        "# post: pad or truncate after sentence.\n",
        "# pre: pad or truncate before sentence.\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "\n",
        "padded = pad_sequences(sequences,\n",
        "                       maxlen=max_length, \n",
        "                       padding=padding_type, \n",
        "                       truncating=trunc_type)\n",
        "\n",
        "# tokenize and pad test sentences\n",
        "# thse will be used later on the model for accuracy test\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_test_padded    = pad_sequences(X_test_sequences,\n",
        "                                 maxlen=max_length, \n",
        "                                 padding=padding_type, \n",
        "                                 truncating=trunc_type)\n",
        "\n",
        "# create the reverse word index\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "# create the decoder\n",
        "def text_decoder(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "# Build network\n",
        "embedding_dim = 100\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(100, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "num_epochs = 10\n",
        "model.fit(padded, \n",
        "          y_train, \n",
        "          epochs=num_epochs, \n",
        "          validation_data=(X_test_padded, \n",
        "                           y_test))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-05 14:00:57--  https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.198.101, 173.194.198.139, 173.194.198.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.198.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-08-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2165dbcvpk79oe7d5ehh6gu8nfjt6hao/1593957600000/11118900490791463723/*/13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-07-05 14:00:57--  https://doc-08-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2165dbcvpk79oe7d5ehh6gu8nfjt6hao/1593957600000/11118900490791463723/*/13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P\n",
            "Resolving doc-08-ak-docs.googleusercontent.com (doc-08-ak-docs.googleusercontent.com)... 74.125.124.132, 2607:f8b0:4001:c14::84\n",
            "Connecting to doc-08-ak-docs.googleusercontent.com (doc-08-ak-docs.googleusercontent.com)|74.125.124.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 127831 (125K) [text/csv]\n",
            "Saving to: ‘/tmp/sentiment.csv’\n",
            "\n",
            "\r/tmp/sentiment.csv    0%[                    ]       0  --.-KB/s               \r/tmp/sentiment.csv  100%[===================>] 124.83K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2020-07-05 14:00:58 (106 MB/s) - ‘/tmp/sentiment.csv’ saved [127831/127831]\n",
            "\n",
            "fatal: destination path 'atalaia' already exists and is not an empty directory.\n",
            "/content/atalaia\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (3.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (4.41.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.5.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (1.0.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (0.22.2.post1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 5)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 5)) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (2.4.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (0.15.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->-r requirements.txt (line 8)) (2.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (2.23.0)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (1.14.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (2.9)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.9 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (1.17.9)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.9->boto3->smart-open>=1.2.1->gensim->-r requirements.txt (line 8)) (0.15.2)\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing Atalaia.egg-info/PKG-INFO\n",
            "writing dependency_links to Atalaia.egg-info/dependency_links.txt\n",
            "writing top-level names to Atalaia.egg-info/top_level.txt\n",
            "reading manifest template 'MANIFEST.in'\n",
            "writing manifest file 'Atalaia.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/atalaia\n",
            "copying build/lib/atalaia/vectors.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "copying build/lib/atalaia/__init__.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "creating build/bdist.linux-x86_64/egg/atalaia/assets\n",
            "copying build/lib/atalaia/assets/stopwords.py -> build/bdist.linux-x86_64/egg/atalaia/assets\n",
            "copying build/lib/atalaia/assets/__init__.py -> build/bdist.linux-x86_64/egg/atalaia/assets\n",
            "copying build/lib/atalaia/assets/contractions.py -> build/bdist.linux-x86_64/egg/atalaia/assets\n",
            "copying build/lib/atalaia/strings.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "copying build/lib/atalaia/files.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "copying build/lib/atalaia/atalaia.py -> build/bdist.linux-x86_64/egg/atalaia\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/vectors.py to vectors.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/assets/stopwords.py to stopwords.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/assets/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/assets/contractions.py to contractions.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/strings.py to strings.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/files.py to files.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/atalaia/atalaia.py to atalaia.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Atalaia.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Atalaia.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Atalaia.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Atalaia.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating 'dist/Atalaia-0.3.0-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing Atalaia-0.3.0-py3.6.egg\n",
            "Removing /usr/local/lib/python3.6/dist-packages/Atalaia-0.3.0-py3.6.egg\n",
            "Copying Atalaia-0.3.0-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Atalaia 0.3.0 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/Atalaia-0.3.0-py3.6.egg\n",
            "Processing dependencies for Atalaia==0.3.0\n",
            "Finished processing dependencies for Atalaia==0.3.0\n",
            "Epoch 1/10\n",
            "50/50 [==============================] - 1s 10ms/step - loss: 0.6969 - accuracy: 0.4918 - val_loss: 0.6864 - val_accuracy: 0.5500\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.6491 - accuracy: 0.7004 - val_loss: 0.6256 - val_accuracy: 0.6650\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.4678 - accuracy: 0.8323 - val_loss: 0.5132 - val_accuracy: 0.7475\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.2669 - accuracy: 0.9227 - val_loss: 0.4522 - val_accuracy: 0.7900\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.1510 - accuracy: 0.9585 - val_loss: 0.4480 - val_accuracy: 0.8050\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0886 - accuracy: 0.9805 - val_loss: 0.4590 - val_accuracy: 0.7975\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0557 - accuracy: 0.9893 - val_loss: 0.4831 - val_accuracy: 0.8000\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0373 - accuracy: 0.9943 - val_loss: 0.4992 - val_accuracy: 0.7875\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0270 - accuracy: 0.9962 - val_loss: 0.5253 - val_accuracy: 0.7925\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0194 - accuracy: 0.9956 - val_loss: 0.5452 - val_accuracy: 0.7875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f48b04c02e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoMbZ5BB9zqv",
        "colab_type": "text"
      },
      "source": [
        "## Save the weights and metadata\n",
        "\n",
        "Now,we can save the metada and the vectors which will be used by the Embedding Projector. \n",
        "\n",
        "This part is very well explained on this [notebook](https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l09c04_nlp_embeddings_and_sentiment.ipynb#scrollTo=g-Q6ALywmWVz)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jkc-8aHGAnZF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c2ef046-a865-4a10-be8c-08f3a6e9ca05"
      },
      "source": [
        "# get the weights of the embedding layer\n",
        "e = model.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "print(weights.shape) # shape: (vocab_size, embedding_dim)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1277, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCOj3XgrTgCx",
        "colab_type": "text"
      },
      "source": [
        "We will save the files on a directory called \"logs\". Let's create it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u_IIlIRJ9yS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cce7c49d-e6f9-4375-dcc2-0736f6d13ee6"
      },
      "source": [
        "# create a logs directory \n",
        "%mkdir ../logs\n",
        "%cd .."
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k99mPf_DTlJj",
        "colab_type": "text"
      },
      "source": [
        "Now, we save them..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G44XjJ1QjjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write the embedding vectors and metadata\n",
        "out_v = io.open('logs/vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('logs/meta.tsv', 'w', encoding='utf-8')\n",
        "for word_num in range(1, vocab_size):\n",
        "  word = reverse_word_index[word_num]\n",
        "  embeddings = weights[word_num]\n",
        "  out_m.write(word + \"\\n\")\n",
        "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wcX_oWmTzmj",
        "colab_type": "text"
      },
      "source": [
        "And download them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pixwe5j9JsOC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "bb29549b-c46c-40e7-c1b1-40245e2dcac6"
      },
      "source": [
        "# Download the files\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  files.download('logs/vecs.tsv')\n",
        "  files.download('logs/meta.tsv')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_6bfd9962-70b1-4ca6-b93f-c3a4f5389d57\", \"vecs.tsv\", 1546229)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_f4ef611a-8b1b-41e2-968c-577211f119f5\", \"meta.tsv\", 8649)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfzSMUbeRQPq",
        "colab_type": "text"
      },
      "source": [
        "Go to [http://projector.tensorflow.org/](http://projector.tensorflow.org/) and load these files :)"
      ]
    }
  ]
}